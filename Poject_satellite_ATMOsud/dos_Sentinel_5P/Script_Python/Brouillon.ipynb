{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILISATEUR = {\n",
    "    \"Moussa\" :  {\"USER\" : \"moussa.dieng@atmosud.org\", \"password\" : \"XM:~G'%SL>26pr2\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des produits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T122838_20240301T141009_33073_03_020600_20240303T021827.nc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 198M/198M [02:27<00:00, 1.41MiB/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ©lÃ©chargement terminÃ© : N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T122838_20240301T141009_33073_03_020600_20240303T021827.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T104708_20240301T122838_33072_03_020600_20240303T003853.nc: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 212M/212M [02:41<00:00, 1.38MiB/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TÃ©lÃ©chargement terminÃ© : N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T104708_20240301T122838_33072_03_020600_20240303T003853.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from getpass import getpass\n",
    "from tqdm import tqdm\n",
    "#import zipfile\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# DÃ©finir les paramÃ¨tres personnalisÃ©s\n",
    "POLLUANT = \"AER_AI\"  #  \"O3\"   #\"NO2\"\n",
    "ZONE = \"POLYGON((4.15 43.01, 7.73 43.01, 7.73 44.76, 4.15 44.76, 4.15 43.01))\"\n",
    "START_DATE = \"2024-03-01T00:00:00.000Z\"\n",
    "END_DATE = \"2024-03-01T23:59:59.999Z\"\n",
    "\n",
    "# DÃ©finir les informations d'utilisateur\n",
    "\n",
    "USERNAME = UTILISATEUR[\"Moussa\"][\"USER\"]\n",
    "password = UTILISATEUR[\"Moussa\"][\"password\"]\n",
    "\n",
    "#USERNAME = \"moussa.dieng@atmosud.org\"\n",
    "#password = \"XM:~G'%SL>26pr2\"      # mot de passs compte COPERNICUS password = \"XM:~G'%SL>26pr2\"\n",
    "def get_keycloak(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\"\n",
    "    }\n",
    "    r = requests.post(\"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\", data=data)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"access_token\"], r.json()[\"refresh_token\"]\n",
    "\n",
    "def refresh_keycloak(refresh_token: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"grant_type\": \"refresh_token\",\n",
    "        \"refresh_token\": refresh_token\n",
    "    }\n",
    "    r = requests.post(\"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\", data=data)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"access_token\"], r.json()[\"refresh_token\"]\n",
    "\n",
    "#passwd = str(getpass())\n",
    "passwd = password \n",
    "keycloak_token, refresh_token = get_keycloak(USERNAME, passwd)\n",
    "\n",
    "def download_zip(url: str, fname: str, chunk_size=1024):\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'Authorization': 'Bearer {}'.format(keycloak_token)})\n",
    "\n",
    "    resp = requests.get(url, allow_redirects=False)\n",
    "    while resp.status_code in (301, 302, 303, 307):\n",
    "        url = resp.headers['Location']\n",
    "        resp = session.get(url, verify=True, stream=True, allow_redirects=False)\n",
    "    \n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(fname, 'wb') as file, tqdm(desc=fname, total=total, unit='iB', unit_scale=True, unit_divisor=1024) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "print(\"Recherche des produits...\")\n",
    "\n",
    "if POLLUANT == \"AER_AI\":\n",
    "   products_url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?&$filter=((Collection/Name%20eq%20%27SENTINEL-5P%27%20and%20(Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27instrumentShortName%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27TROPOMI%27)%20and%20(contains(Name,%27L2__{POLLUANT}_%27)%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;{ZONE}%27)))%20and%20Online%20eq%20true)%20and%20ContentDate/Start%20ge%20{START_DATE}%20and%20ContentDate/Start%20lt%20{END_DATE})&$orderby=ContentDate/Start%20desc&$expand=Attributes&$count=True&$top=1000&$expand=Assets&$skip=0\"\n",
    "else :\n",
    "   products_url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?&$filter=((Collection/Name%20eq%20%27SENTINEL-5P%27%20and%20(Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27instrumentShortName%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27TROPOMI%27)%20and%20(contains(Name,%27L2__{POLLUTANT}___%27)%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;{ZONE}%27)))%20and%20Online%20eq%20true)%20and%20ContentDate/Start%20ge%20{START_DATE}%20and%20ContentDate/Start%20lt%20{END_DATE})&$orderby=ContentDate/Start%20desc&$expand=Attributes&$count=True&$top=1000&$expand=Assets&$skip=0\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({'Authorization': 'Bearer {}'.format(keycloak_token)})\n",
    "response = requests.get(products_url, headers=session.headers)\n",
    "\n",
    "try:\n",
    "    lines = response.json()\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Erreur : Impossible de dÃ©coder la rÃ©ponse JSON.\")\n",
    "    exit(1)\n",
    "\n",
    "if \"value\" not in lines:\n",
    "    print(\"Erreur : La clÃ© 'value' est absente de la rÃ©ponse API.\")\n",
    "    print(\"RÃ©ponse API :\", lines)\n",
    "    exit(1)\n",
    "\n",
    "# Chemin de stockage automatiquement en fonction du polluant\n",
    "destination_root =f\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/{POLLUTANT}\"\n",
    "if not os.path.exists(destination_root):\n",
    "    os.makedirs(destination_root)\n",
    "\n",
    "for value_data in lines[\"value\"]:\n",
    "    product_name = value_data[\"Name\"]\n",
    "    product_identyficator = str(value_data['Id'])\n",
    "\n",
    "    match = re.search(r\"(\\d{8}T\\d{6})\", product_name)\n",
    "    if not match:\n",
    "        print(f\"Erreur : Impossible d'extraire la date du fichier {product_name}\")\n",
    "        continue\n",
    "\n",
    "    date_str = match.group(1)\n",
    "    date_obj = datetime.strptime(date_str, \"%Y%m%dT%H%M%S\")\n",
    "    folder_name = f\"{date_obj.year}_{date_obj.month:02d}\"\n",
    "    destination_folder = os.path.join(destination_root, folder_name)\n",
    "\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    file_path = os.path.join(destination_folder, product_name)\n",
    "\n",
    "    url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products({product_identyficator})/$value\"\n",
    "    keycloak_token, refresh_token = refresh_keycloak(refresh_token)\n",
    "\n",
    "    download_zip(url, file_path)\n",
    "\n",
    "    print(f\"TÃ©lÃ©chargement terminÃ© : {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traitement des donnÃ©es et Rasters moyens Journaliers, mensuels ou annuels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import cKDTree\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "\n",
    "def create_parallelogram_grid(lat_min, lat_max, lon_min, lon_max, dx, dy):\n",
    "    \"\"\"GÃ©nÃ¨re une grille de parallÃ©logrammes couvrant la rÃ©gion spÃ©cifiÃ©e.\"\"\"\n",
    "    polygons, centers = [], []\n",
    "\n",
    "    for x in np.arange(lon_min, lon_max, dx):\n",
    "        for y in np.arange(lat_min, lat_max, dy):\n",
    "            p = Polygon([\n",
    "                (x, y), (x + dx, y), (x + dx + dy / 2, y + dy), (x + dy / 2, y + dy)\n",
    "            ])\n",
    "            polygons.append(p)\n",
    "            centers.append(Point(x + dx / 2, y + dy / 2))\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=polygons), centers\n",
    "\n",
    "def process_netcdf_to_raster(year: str, pollutant: str, base_dir: str, qa_min: float = 0.75, month: str = None, day: str = None):\n",
    "    \"\"\"\n",
    "    GÃ©nÃ¨re un raster journalier, mensuel ou annuel en combinant les fichiers NetCDF.\n",
    "\n",
    "    :param year: AnnÃ©e \n",
    "    :param pollutant: Polluant Ã  traiter (ex: \"NO2\")\n",
    "    :param base_dir: RÃ©pertoire de base contenant les fichiers NetCDF\n",
    "    :param month: Mois (ex: \"07\") ou None pour l'annÃ©e entiÃ¨re\n",
    "    :param day: Jour (ex: \"02\") ou None pour le mois entier\n",
    "    \"\"\"\n",
    "    pollutant = pollutant.upper()\n",
    "    if pollutant not in POLLUANTS_CONFIG:\n",
    "        print(f\" Polluant '{pollutant}' non reconnu.\")\n",
    "        return\n",
    "\n",
    "    var_name = POLLUANTS_CONFIG[pollutant][\"var\"]\n",
    "    periods = [f\"{year}_{month}\"] if month else [f\"{year}_{m:02d}\" for m in range(1, 13)]\n",
    "    \n",
    "    # DÃ©terminer le fichier de sortie\n",
    "    if day:\n",
    "        output_name = f\"{pollutant}_mean_{year}_{month}_{day}.tif\"\n",
    "        print(f\" GÃ©nÃ©ration du raster journalier ({day}/{month}/{year})\")\n",
    "    elif month:\n",
    "        output_name = f\"{pollutant}_mean_{year}_{month}.tif\"\n",
    "        print(f\" GÃ©nÃ©ration du raster mensuel ({month}/{year})\")\n",
    "    else:\n",
    "        output_name = f\"{pollutant}_mean_{year}.tif\"\n",
    "        print(f\" GÃ©nÃ©ration du raster annuel ({year})\")\n",
    "\n",
    "    # DÃ©finition de la grille\n",
    "    lat_min, lat_max = 42.3, 45.4\n",
    "    lon_min, lon_max = 3.63, 8\n",
    "    resolution_x, resolution_y = 0.1, 0.06\n",
    "    grid, grid_centers = create_parallelogram_grid(lat_min, lat_max, lon_min, lon_max, resolution_x, resolution_y)\n",
    "\n",
    "    # Initialisation des matrices\n",
    "    sum_values = np.zeros(len(grid), dtype=np.float32)\n",
    "    count_values = np.zeros(len(grid), dtype=np.int32)\n",
    "\n",
    "    # Recherche des fichiers NetCDF\n",
    "    files = []\n",
    "    for period in periods:\n",
    "        input_dir = os.path.join(base_dir, pollutant, period)\n",
    "        if not os.path.exists(input_dir):\n",
    "            continue\n",
    "        files.extend([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".nc\")])\n",
    "\n",
    "    if not files:\n",
    "        print(f\" Aucun fichier NetCDF trouvÃ© pour {pollutant} en {year} ({month if month else 'AnnÃ©e entiÃ¨re'})\")\n",
    "        return\n",
    "\n",
    "    for file in files:\n",
    "        if day and day not in file:\n",
    "            continue\n",
    "\n",
    "        ds = xr.open_dataset(file, group=\"PRODUCT\")\n",
    "        data_array = ds[var_name].isel(time=0).values.astype(np.float32)\n",
    "        qa = ds[\"qa_value\"].isel(time=0).values.astype(np.float32)\n",
    "        lat = ds[\"latitude\"].values.astype(np.float32)\n",
    "        lon = ds[\"longitude\"].values.astype(np.float32)\n",
    "        ds.close()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"lat\": lat.ravel(),\n",
    "            \"lon\": lon.ravel(),\n",
    "            \"value\": data_array.ravel(),\n",
    "            \"qa_value\": qa.ravel()\n",
    "        })\n",
    "        df = df[df[\"qa_value\"] >=qa_min]\n",
    "        df = df[(df[\"lat\"] >= lat_min) & (df[\"lat\"] <= lat_max) & \n",
    "                (df[\"lon\"] >= lon_min) & (df[\"lon\"] <= lon_max)]\n",
    "\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        tree = cKDTree(np.array([[p.x, p.y] for p in grid_centers]))\n",
    "        indices = tree.query(df[['lon', 'lat']].values)[1]\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            sum_values[idx] += df['value'].iloc[i]\n",
    "            count_values[idx] += 1\n",
    "\n",
    "    # Calcul de la moyenne\n",
    "    mean_values = np.full(len(grid), np.nan, dtype=np.float32)\n",
    "    valid_cells = count_values > 0\n",
    "    mean_values[valid_cells] = sum_values[valid_cells] / count_values[valid_cells]\n",
    "    grid[\"mean_value\"] = mean_values\n",
    "\n",
    "    # Export en raster\n",
    "    raster_size_x = len(np.arange(lon_min, lon_max, resolution_x))\n",
    "    raster_size_y = len(np.arange(lat_min, lat_max, resolution_y))\n",
    "    transform = from_origin(lon_min, lat_max, resolution_x, resolution_y)\n",
    "\n",
    "    raster_data = np.full((raster_size_y, raster_size_x), np.nan, dtype=np.float32)\n",
    "    for idx, row in grid.iterrows():\n",
    "        poly = row.geometry\n",
    "        x_idx = int((poly.centroid.x - lon_min) / resolution_x)\n",
    "        y_idx = int((lat_max - poly.centroid.y) / resolution_y)\n",
    "        if 0 <= x_idx < raster_size_x and 0 <= y_idx < raster_size_y:\n",
    "            raster_data[y_idx, x_idx] = row[\"mean_value\"]\n",
    "\n",
    "    # Stockage dans output_rasters/{polluant}\n",
    "    output_dir = os.path.join(base_dir, pollutant, \"output_rasters\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_name)\n",
    "\n",
    "    with rasterio.open(output_path, 'w', driver='GTiff', height=raster_size_y, width=raster_size_x,\n",
    "                       count=1, dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "        dst.write(raster_data, 1)\n",
    "\n",
    "    print(f\" Raster enregistrÃ© sous {output_path}\")\n",
    "\n",
    "# ðŸ”¹ Exemples d'utilisation :\n",
    "\n",
    "base_dir = \"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output\"\n",
    "process_netcdf_to_raster(\"2024\", \"AER_AI\", base_dir, month=\"03\", day=\"01\")  # Raster journalier\n",
    "#process_netcdf_to_raster(\"2024\", \"NO2\", base_dir, month=\"07\")            # Raster mensuel\n",
    "#process_netcdf_to_raster(\"2024\", \"NO2\", base_dir)                        # Raster annuel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import box\n",
    "\n",
    "\n",
    "\n",
    "def visualize_raster(date, polluant=\"NO2\", raster_dir=None, shp_path=None):\n",
    "    \"\"\"\n",
    "    Visualise les rasters journaliers, mensuels ou annuels.\n",
    "\n",
    "    :param date: Date au format YYYY, YYYYMM ou YYYYMMDD\n",
    "    :param pollutant: Nom du polluant (ex: \"NO2\")\n",
    "    :param base_dir: RÃ©pertoire de base contenant les rasters\n",
    "    :param shp_path: Chemin vers un shapefile pour superposer les limites rÃ©gionales\n",
    "    \"\"\"\n",
    "    if raster_dir is None:\n",
    "        raster_dir =  f\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/{polluant}/output_rasters\"\n",
    "    \n",
    "    # VÃ©rification du polluant\n",
    "    polluant = polluant.upper()\n",
    "    if polluant not in POLLUANTS_CONFIG:\n",
    "        print(f\"Polluant '{polluant}' non reconnu.\")\n",
    "        return\n",
    "\n",
    "    # DÃ©terminer le type de date\n",
    "    if len(date) == 8:  # Journalier\n",
    "        year, month, day = date[:4], date[4:6], date[6:8]\n",
    "        raster_filename = f\"{polluant}_mean_{year}_{month}_{day}.tif\"\n",
    "    elif len(date) == 6:  # Mensuel\n",
    "        year, month = date[:4], date[4:6]\n",
    "        raster_filename = f\"{polluant}_mean_{year}_{month}.tif\"\n",
    "    elif len(date) == 4:  # Annuel\n",
    "        year = date[:4]\n",
    "        raster_filename = f\"{polluant}_mean_{year}.tif\"\n",
    "    else:\n",
    "        print(\"Format de date invalide. Utilisez YYYY, YYYYMM ou YYYYMMDD.\")\n",
    "        return\n",
    "\n",
    "    # Construire le chemin complet du fichier raster\n",
    "    raster_path = os.path.join(raster_dir, raster_filename)\n",
    "\n",
    "    # VÃ©rification de l'existence du fichier raster\n",
    "    if not os.path.exists(raster_path):\n",
    "        print(f\"Fichier raster introuvable : {raster_path}\")\n",
    "        print(\"VÃ©rifiez le rÃ©pertoire ou le format de la date.\")\n",
    "        return\n",
    "\n",
    "    # Charger le shapefile (facultatif)\n",
    "    if shp_path and os.path.exists(shp_path):\n",
    "        shapefile = gpd.read_file(shp_path)\n",
    "    else:\n",
    "        shapefile = None\n",
    "\n",
    "    # Charger le raster\n",
    "    try:\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            raster_data = src.read(1)  # PremiÃ¨re bande du raster\n",
    "            raster_bounds = src.bounds  # Bornes du raster\n",
    "            extent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n",
    "\n",
    "            # Masquer les valeurs nodata\n",
    "            raster_data = np.ma.masked_where(raster_data == src.nodata, raster_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du raster : {e}\")\n",
    "        return\n",
    "\n",
    "    # Visualisation\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    img = ax.imshow(raster_data, extent=extent, cmap=\"viridis\", origin=\"upper\")\n",
    "\n",
    "    # Ajouter un shapefile (si disponible)\n",
    "    if shapefile is not None:\n",
    "        shapefile.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=1, label=\"Limites rÃ©gionales\")\n",
    "\n",
    "    # Ajouter une barre de couleur\n",
    "    unite_name = POLLUANTS_CONFIG[polluant][\"unite\"]\n",
    "    cbar = plt.colorbar(img, ax=ax, orientation=\"vertical\", fraction=0.03, pad=0.04)\n",
    "    cbar.set_label(f\"Concentration de {polluant} ({POLLUANTS_CONFIG[polluant][\"unite\"]}) \")\n",
    "\n",
    "    # Reformater la date au format jj-mm-aa\n",
    "    date = datetime.strptime(date, \"%Y%m%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    # ParamÃ¨tres de l'intrigue\n",
    "    \n",
    "    ax.set_title(f\"Concentration de {polluant} au {date}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    if shapefile is not None:\n",
    "        ax.legend()\n",
    "\n",
    "    \n",
    "   # CrÃ©er un rÃ©pertoire \"graphes\" dans le rÃ©pertoire du polluant\n",
    "    graph_dir = f\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/{polluant}/graphes\"\n",
    "    os.makedirs(graph_dir, exist_ok=True)  # CrÃ©er le dossier s'il n'existe pas\n",
    "\n",
    "    # GÃ©nÃ©rer un nom de fichier unique pour le graphique\n",
    "    graph_filename = f\"{polluant}_{date}.png\"\n",
    "    graph_path = os.path.join(graph_dir, graph_filename)\n",
    "\n",
    "    # Sauvegarder la figure\n",
    "    plt.savefig(graph_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Graphique sauvegardÃ© dans : {graph_path}\")\n",
    "    # Afficher la carte\n",
    "    plt.show()\n",
    "    plt.close(fig)  # Fermer la figure pour libÃ©rer la mÃ©moire\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "# Visualiser un raster journalier\n",
    "\n",
    "shp_path=\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/Stage_Moussa/fichiers_shp/contour_region_4326.shp\"\n",
    "base_dir=\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output\"\n",
    "visualize_raster(date=\"20240802\", polluant=\"NO2\", shp_path=shp_path)\n",
    "# Visualiser un raster mensuel\n",
    "#visualize_raster(\"202407\", polluant=\"NO2\",shp_path=shp_path)\n",
    "\n",
    "# Visualiser un raster annuel\n",
    "# visualize_raster(\"2024\", pollutant=\"NO2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
