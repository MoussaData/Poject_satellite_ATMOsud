{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTILISATEUR = {\n",
    "    \"Moussa\" :  {\"USER\" : \"moussa.dieng@atmosud.org\", \"password\" : \"XM:~G'%SL>26pr2\"}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recherche des produits...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T122838_20240301T141009_33073_03_020600_20240303T021827.nc: 100%|██████████| 198M/198M [02:27<00:00, 1.41MiB/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement terminé : N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T122838_20240301T141009_33073_03_020600_20240303T021827.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T104708_20240301T122838_33072_03_020600_20240303T003853.nc: 100%|██████████| 212M/212M [02:41<00:00, 1.38MiB/s]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Téléchargement terminé : N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/AER_AI\\2024_03\\S5P_OFFL_L2__AER_AI_20240301T104708_20240301T122838_33072_03_020600_20240303T003853.nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import os\n",
    "from getpass import getpass\n",
    "from tqdm import tqdm\n",
    "#import zipfile\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "# Définir les paramètres personnalisés\n",
    "POLLUANT = \"AER_AI\"  #  \"O3\"   #\"NO2\"\n",
    "ZONE = \"POLYGON((4.15 43.01, 7.73 43.01, 7.73 44.76, 4.15 44.76, 4.15 43.01))\"\n",
    "START_DATE = \"2024-03-01T00:00:00.000Z\"\n",
    "END_DATE = \"2024-03-01T23:59:59.999Z\"\n",
    "\n",
    "# Définir les informations d'utilisateur\n",
    "\n",
    "USERNAME = UTILISATEUR[\"Moussa\"][\"USER\"]\n",
    "password = UTILISATEUR[\"Moussa\"][\"password\"]\n",
    "\n",
    "#USERNAME = \"moussa.dieng@atmosud.org\"\n",
    "#password = \"XM:~G'%SL>26pr2\"      # mot de passs compte COPERNICUS password = \"XM:~G'%SL>26pr2\"\n",
    "def get_keycloak(username: str, password: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"username\": username,\n",
    "        \"password\": password,\n",
    "        \"grant_type\": \"password\"\n",
    "    }\n",
    "    r = requests.post(\"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\", data=data)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"access_token\"], r.json()[\"refresh_token\"]\n",
    "\n",
    "def refresh_keycloak(refresh_token: str) -> str:\n",
    "    data = {\n",
    "        \"client_id\": \"cdse-public\",\n",
    "        \"grant_type\": \"refresh_token\",\n",
    "        \"refresh_token\": refresh_token\n",
    "    }\n",
    "    r = requests.post(\"https://identity.dataspace.copernicus.eu/auth/realms/CDSE/protocol/openid-connect/token\", data=data)\n",
    "    r.raise_for_status()\n",
    "    return r.json()[\"access_token\"], r.json()[\"refresh_token\"]\n",
    "\n",
    "#passwd = str(getpass())\n",
    "passwd = password \n",
    "keycloak_token, refresh_token = get_keycloak(USERNAME, passwd)\n",
    "\n",
    "def download_zip(url: str, fname: str, chunk_size=1024):\n",
    "    session = requests.Session()\n",
    "    session.headers.update({'Authorization': 'Bearer {}'.format(keycloak_token)})\n",
    "\n",
    "    resp = requests.get(url, allow_redirects=False)\n",
    "    while resp.status_code in (301, 302, 303, 307):\n",
    "        url = resp.headers['Location']\n",
    "        resp = session.get(url, verify=True, stream=True, allow_redirects=False)\n",
    "    \n",
    "    total = int(resp.headers.get('content-length', 0))\n",
    "    with open(fname, 'wb') as file, tqdm(desc=fname, total=total, unit='iB', unit_scale=True, unit_divisor=1024) as bar:\n",
    "        for data in resp.iter_content(chunk_size=chunk_size):\n",
    "            size = file.write(data)\n",
    "            bar.update(size)\n",
    "\n",
    "print(\"Recherche des produits...\")\n",
    "\n",
    "if POLLUANT == \"AER_AI\":\n",
    "   products_url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?&$filter=((Collection/Name%20eq%20%27SENTINEL-5P%27%20and%20(Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27instrumentShortName%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27TROPOMI%27)%20and%20(contains(Name,%27L2__{POLLUANT}_%27)%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;{ZONE}%27)))%20and%20Online%20eq%20true)%20and%20ContentDate/Start%20ge%20{START_DATE}%20and%20ContentDate/Start%20lt%20{END_DATE})&$orderby=ContentDate/Start%20desc&$expand=Attributes&$count=True&$top=1000&$expand=Assets&$skip=0\"\n",
    "else :\n",
    "   products_url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products?&$filter=((Collection/Name%20eq%20%27SENTINEL-5P%27%20and%20(Attributes/OData.CSC.StringAttribute/any(att:att/Name%20eq%20%27instrumentShortName%27%20and%20att/OData.CSC.StringAttribute/Value%20eq%20%27TROPOMI%27)%20and%20(contains(Name,%27L2__{POLLUTANT}___%27)%20and%20OData.CSC.Intersects(area=geography%27SRID=4326;{ZONE}%27)))%20and%20Online%20eq%20true)%20and%20ContentDate/Start%20ge%20{START_DATE}%20and%20ContentDate/Start%20lt%20{END_DATE})&$orderby=ContentDate/Start%20desc&$expand=Attributes&$count=True&$top=1000&$expand=Assets&$skip=0\"\n",
    "\n",
    "session = requests.Session()\n",
    "session.headers.update({'Authorization': 'Bearer {}'.format(keycloak_token)})\n",
    "response = requests.get(products_url, headers=session.headers)\n",
    "\n",
    "try:\n",
    "    lines = response.json()\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Erreur : Impossible de décoder la réponse JSON.\")\n",
    "    exit(1)\n",
    "\n",
    "if \"value\" not in lines:\n",
    "    print(\"Erreur : La clé 'value' est absente de la réponse API.\")\n",
    "    print(\"Réponse API :\", lines)\n",
    "    exit(1)\n",
    "\n",
    "# Chemin de stockage automatiquement en fonction du polluant\n",
    "destination_root =f\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/{POLLUTANT}\"\n",
    "if not os.path.exists(destination_root):\n",
    "    os.makedirs(destination_root)\n",
    "\n",
    "for value_data in lines[\"value\"]:\n",
    "    product_name = value_data[\"Name\"]\n",
    "    product_identyficator = str(value_data['Id'])\n",
    "\n",
    "    match = re.search(r\"(\\d{8}T\\d{6})\", product_name)\n",
    "    if not match:\n",
    "        print(f\"Erreur : Impossible d'extraire la date du fichier {product_name}\")\n",
    "        continue\n",
    "\n",
    "    date_str = match.group(1)\n",
    "    date_obj = datetime.strptime(date_str, \"%Y%m%dT%H%M%S\")\n",
    "    folder_name = f\"{date_obj.year}_{date_obj.month:02d}\"\n",
    "    destination_folder = os.path.join(destination_root, folder_name)\n",
    "\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    file_path = os.path.join(destination_folder, product_name)\n",
    "\n",
    "    url = f\"https://catalogue.dataspace.copernicus.eu/odata/v1/Products({product_identyficator})/$value\"\n",
    "    keycloak_token, refresh_token = refresh_keycloak(refresh_token)\n",
    "\n",
    "    download_zip(url, file_path)\n",
    "\n",
    "    print(f\"Téléchargement terminé : {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Traitement des données et Rasters moyens Journaliers, mensuels ou annuels**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import rasterio\n",
    "import xarray as xr\n",
    "import pandas as pd\n",
    "from shapely.geometry import Polygon, Point\n",
    "import geopandas as gpd\n",
    "from scipy.spatial import cKDTree\n",
    "from rasterio.transform import from_origin\n",
    "\n",
    "\n",
    "def create_parallelogram_grid(lat_min, lat_max, lon_min, lon_max, dx, dy):\n",
    "    \"\"\"Génère une grille de parallélogrammes couvrant la région spécifiée.\"\"\"\n",
    "    polygons, centers = [], []\n",
    "\n",
    "    for x in np.arange(lon_min, lon_max, dx):\n",
    "        for y in np.arange(lat_min, lat_max, dy):\n",
    "            p = Polygon([\n",
    "                (x, y), (x + dx, y), (x + dx + dy / 2, y + dy), (x + dy / 2, y + dy)\n",
    "            ])\n",
    "            polygons.append(p)\n",
    "            centers.append(Point(x + dx / 2, y + dy / 2))\n",
    "\n",
    "    return gpd.GeoDataFrame(geometry=polygons), centers\n",
    "\n",
    "def process_netcdf_to_raster(year: str, pollutant: str, base_dir: str, qa_min: float = 0.75, month: str = None, day: str = None):\n",
    "    \"\"\"\n",
    "    Génère un raster journalier, mensuel ou annuel en combinant les fichiers NetCDF.\n",
    "\n",
    "    :param year: Année \n",
    "    :param pollutant: Polluant à traiter (ex: \"NO2\")\n",
    "    :param base_dir: Répertoire de base contenant les fichiers NetCDF\n",
    "    :param month: Mois (ex: \"07\") ou None pour l'année entière\n",
    "    :param day: Jour (ex: \"02\") ou None pour le mois entier\n",
    "    \"\"\"\n",
    "    pollutant = pollutant.upper()\n",
    "    if pollutant not in POLLUANTS_CONFIG:\n",
    "        print(f\" Polluant '{pollutant}' non reconnu.\")\n",
    "        return\n",
    "\n",
    "    var_name = POLLUANTS_CONFIG[pollutant][\"var\"]\n",
    "    periods = [f\"{year}_{month}\"] if month else [f\"{year}_{m:02d}\" for m in range(1, 13)]\n",
    "    \n",
    "    # Déterminer le fichier de sortie\n",
    "    if day:\n",
    "        output_name = f\"{pollutant}_mean_{year}_{month}_{day}.tif\"\n",
    "        print(f\" Génération du raster journalier ({day}/{month}/{year})\")\n",
    "    elif month:\n",
    "        output_name = f\"{pollutant}_mean_{year}_{month}.tif\"\n",
    "        print(f\" Génération du raster mensuel ({month}/{year})\")\n",
    "    else:\n",
    "        output_name = f\"{pollutant}_mean_{year}.tif\"\n",
    "        print(f\" Génération du raster annuel ({year})\")\n",
    "\n",
    "    # Définition de la grille\n",
    "    lat_min, lat_max = 42.3, 45.4\n",
    "    lon_min, lon_max = 3.63, 8\n",
    "    resolution_x, resolution_y = 0.1, 0.06\n",
    "    grid, grid_centers = create_parallelogram_grid(lat_min, lat_max, lon_min, lon_max, resolution_x, resolution_y)\n",
    "\n",
    "    # Initialisation des matrices\n",
    "    sum_values = np.zeros(len(grid), dtype=np.float32)\n",
    "    count_values = np.zeros(len(grid), dtype=np.int32)\n",
    "\n",
    "    # Recherche des fichiers NetCDF\n",
    "    files = []\n",
    "    for period in periods:\n",
    "        input_dir = os.path.join(base_dir, pollutant, period)\n",
    "        if not os.path.exists(input_dir):\n",
    "            continue\n",
    "        files.extend([os.path.join(input_dir, f) for f in os.listdir(input_dir) if f.endswith(\".nc\")])\n",
    "\n",
    "    if not files:\n",
    "        print(f\" Aucun fichier NetCDF trouvé pour {pollutant} en {year} ({month if month else 'Année entière'})\")\n",
    "        return\n",
    "\n",
    "    for file in files:\n",
    "        if day and day not in file:\n",
    "            continue\n",
    "\n",
    "        ds = xr.open_dataset(file, group=\"PRODUCT\")\n",
    "        data_array = ds[var_name].isel(time=0).values.astype(np.float32)\n",
    "        qa = ds[\"qa_value\"].isel(time=0).values.astype(np.float32)\n",
    "        lat = ds[\"latitude\"].values.astype(np.float32)\n",
    "        lon = ds[\"longitude\"].values.astype(np.float32)\n",
    "        ds.close()\n",
    "\n",
    "        df = pd.DataFrame({\n",
    "            \"lat\": lat.ravel(),\n",
    "            \"lon\": lon.ravel(),\n",
    "            \"value\": data_array.ravel(),\n",
    "            \"qa_value\": qa.ravel()\n",
    "        })\n",
    "        df = df[df[\"qa_value\"] >=qa_min]\n",
    "        df = df[(df[\"lat\"] >= lat_min) & (df[\"lat\"] <= lat_max) & \n",
    "                (df[\"lon\"] >= lon_min) & (df[\"lon\"] <= lon_max)]\n",
    "\n",
    "        if df.empty:\n",
    "            continue\n",
    "\n",
    "        tree = cKDTree(np.array([[p.x, p.y] for p in grid_centers]))\n",
    "        indices = tree.query(df[['lon', 'lat']].values)[1]\n",
    "\n",
    "        for i, idx in enumerate(indices):\n",
    "            sum_values[idx] += df['value'].iloc[i]\n",
    "            count_values[idx] += 1\n",
    "\n",
    "    # Calcul de la moyenne\n",
    "    mean_values = np.full(len(grid), np.nan, dtype=np.float32)\n",
    "    valid_cells = count_values > 0\n",
    "    mean_values[valid_cells] = sum_values[valid_cells] / count_values[valid_cells]\n",
    "    grid[\"mean_value\"] = mean_values\n",
    "\n",
    "    # Export en raster\n",
    "    raster_size_x = len(np.arange(lon_min, lon_max, resolution_x))\n",
    "    raster_size_y = len(np.arange(lat_min, lat_max, resolution_y))\n",
    "    transform = from_origin(lon_min, lat_max, resolution_x, resolution_y)\n",
    "\n",
    "    raster_data = np.full((raster_size_y, raster_size_x), np.nan, dtype=np.float32)\n",
    "    for idx, row in grid.iterrows():\n",
    "        poly = row.geometry\n",
    "        x_idx = int((poly.centroid.x - lon_min) / resolution_x)\n",
    "        y_idx = int((lat_max - poly.centroid.y) / resolution_y)\n",
    "        if 0 <= x_idx < raster_size_x and 0 <= y_idx < raster_size_y:\n",
    "            raster_data[y_idx, x_idx] = row[\"mean_value\"]\n",
    "\n",
    "    # Stockage dans output_rasters/{polluant}\n",
    "    output_dir = os.path.join(base_dir, pollutant, \"output_rasters\")\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    output_path = os.path.join(output_dir, output_name)\n",
    "\n",
    "    with rasterio.open(output_path, 'w', driver='GTiff', height=raster_size_y, width=raster_size_x,\n",
    "                       count=1, dtype='float32', crs='EPSG:4326', transform=transform) as dst:\n",
    "        dst.write(raster_data, 1)\n",
    "\n",
    "    print(f\" Raster enregistré sous {output_path}\")\n",
    "\n",
    "# 🔹 Exemples d'utilisation :\n",
    "\n",
    "base_dir = \"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output\"\n",
    "process_netcdf_to_raster(\"2024\", \"AER_AI\", base_dir, month=\"03\", day=\"01\")  # Raster journalier\n",
    "#process_netcdf_to_raster(\"2024\", \"NO2\", base_dir, month=\"07\")            # Raster mensuel\n",
    "#process_netcdf_to_raster(\"2024\", \"NO2\", base_dir)                        # Raster annuel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  **Visualisation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import rasterio\n",
    "import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from rasterio.plot import show\n",
    "from shapely.geometry import box\n",
    "\n",
    "\n",
    "\n",
    "def visualize_raster(date, polluant=\"NO2\", raster_dir=None, shp_path=None):\n",
    "    \"\"\"\n",
    "    Visualise les rasters journaliers, mensuels ou annuels.\n",
    "\n",
    "    :param date: Date au format YYYY, YYYYMM ou YYYYMMDD\n",
    "    :param pollutant: Nom du polluant (ex: \"NO2\")\n",
    "    :param base_dir: Répertoire de base contenant les rasters\n",
    "    :param shp_path: Chemin vers un shapefile pour superposer les limites régionales\n",
    "    \"\"\"\n",
    "    if raster_dir is None:\n",
    "        raster_dir =  f\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/{polluant}/output_rasters\"\n",
    "    \n",
    "    # Vérification du polluant\n",
    "    polluant = polluant.upper()\n",
    "    if polluant not in POLLUANTS_CONFIG:\n",
    "        print(f\"Polluant '{polluant}' non reconnu.\")\n",
    "        return\n",
    "\n",
    "    # Déterminer le type de date\n",
    "    if len(date) == 8:  # Journalier\n",
    "        year, month, day = date[:4], date[4:6], date[6:8]\n",
    "        raster_filename = f\"{polluant}_mean_{year}_{month}_{day}.tif\"\n",
    "    elif len(date) == 6:  # Mensuel\n",
    "        year, month = date[:4], date[4:6]\n",
    "        raster_filename = f\"{polluant}_mean_{year}_{month}.tif\"\n",
    "    elif len(date) == 4:  # Annuel\n",
    "        year = date[:4]\n",
    "        raster_filename = f\"{polluant}_mean_{year}.tif\"\n",
    "    else:\n",
    "        print(\"Format de date invalide. Utilisez YYYY, YYYYMM ou YYYYMMDD.\")\n",
    "        return\n",
    "\n",
    "    # Construire le chemin complet du fichier raster\n",
    "    raster_path = os.path.join(raster_dir, raster_filename)\n",
    "\n",
    "    # Vérification de l'existence du fichier raster\n",
    "    if not os.path.exists(raster_path):\n",
    "        print(f\"Fichier raster introuvable : {raster_path}\")\n",
    "        print(\"Vérifiez le répertoire ou le format de la date.\")\n",
    "        return\n",
    "\n",
    "    # Charger le shapefile (facultatif)\n",
    "    if shp_path and os.path.exists(shp_path):\n",
    "        shapefile = gpd.read_file(shp_path)\n",
    "    else:\n",
    "        shapefile = None\n",
    "\n",
    "    # Charger le raster\n",
    "    try:\n",
    "        with rasterio.open(raster_path) as src:\n",
    "            raster_data = src.read(1)  # Première bande du raster\n",
    "            raster_bounds = src.bounds  # Bornes du raster\n",
    "            extent = [src.bounds.left, src.bounds.right, src.bounds.bottom, src.bounds.top]\n",
    "\n",
    "            # Masquer les valeurs nodata\n",
    "            raster_data = np.ma.masked_where(raster_data == src.nodata, raster_data)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du chargement du raster : {e}\")\n",
    "        return\n",
    "\n",
    "    # Visualisation\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    img = ax.imshow(raster_data, extent=extent, cmap=\"viridis\", origin=\"upper\")\n",
    "\n",
    "    # Ajouter un shapefile (si disponible)\n",
    "    if shapefile is not None:\n",
    "        shapefile.boundary.plot(ax=ax, edgecolor=\"red\", linewidth=1, label=\"Limites régionales\")\n",
    "\n",
    "    # Ajouter une barre de couleur\n",
    "    unite_name = POLLUANTS_CONFIG[polluant][\"unite\"]\n",
    "    cbar = plt.colorbar(img, ax=ax, orientation=\"vertical\", fraction=0.03, pad=0.04)\n",
    "    cbar.set_label(f\"Concentration de {polluant} ({POLLUANTS_CONFIG[polluant][\"unite\"]}) \")\n",
    "\n",
    "    # Reformater la date au format jj-mm-aa\n",
    "    date = datetime.strptime(date, \"%Y%m%d\").strftime(\"%d-%m-%Y\")\n",
    "\n",
    "    # Paramètres de l'intrigue\n",
    "    \n",
    "    ax.set_title(f\"Concentration de {polluant} au {date}\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    if shapefile is not None:\n",
    "        ax.legend()\n",
    "\n",
    "    \n",
    "   # Créer un répertoire \"graphes\" dans le répertoire du polluant\n",
    "    graph_dir = f\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output/{polluant}/graphes\"\n",
    "    os.makedirs(graph_dir, exist_ok=True)  # Créer le dossier s'il n'existe pas\n",
    "\n",
    "    # Générer un nom de fichier unique pour le graphique\n",
    "    graph_filename = f\"{polluant}_{date}.png\"\n",
    "    graph_path = os.path.join(graph_dir, graph_filename)\n",
    "\n",
    "    # Sauvegarder la figure\n",
    "    plt.savefig(graph_path, dpi=300, bbox_inches=\"tight\")\n",
    "    print(f\"Graphique sauvegardé dans : {graph_path}\")\n",
    "    # Afficher la carte\n",
    "    plt.show()\n",
    "    plt.close(fig)  # Fermer la figure pour libérer la mémoire\n",
    "\n",
    "# Exemple d'utilisation :\n",
    "# Visualiser un raster journalier\n",
    "\n",
    "shp_path=\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/Stage_Moussa/fichiers_shp/contour_region_4326.shp\"\n",
    "base_dir=\"N:/MOD_SERVER/SATELLITES/tropomi_s5_annuel/alternance_moussa/output\"\n",
    "visualize_raster(date=\"20240802\", polluant=\"NO2\", shp_path=shp_path)\n",
    "# Visualiser un raster mensuel\n",
    "#visualize_raster(\"202407\", polluant=\"NO2\",shp_path=shp_path)\n",
    "\n",
    "# Visualiser un raster annuel\n",
    "# visualize_raster(\"2024\", pollutant=\"NO2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
